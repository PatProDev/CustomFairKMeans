{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FairKMeans Algorithm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "The `FairKMeans` algorithm is a modification of the classic K-Means clustering algorithm. It aims to cluster data points while ensuring fairness with respect to a given sensitive feature (e.g., gender, race). Fairness is enforced by constraining the sensitive group ratios in each cluster to be close to their global proportions.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is useful in applications where it's important to ensure that sensitive groups are fairly represented within each cluster.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Fairness Constraint**: Ensures that the ratio of sensitive groups in each cluster adheres to a global ratio within a specified tolerance.\n",
    "2. **Dynamic Tolerance**: Starts with a relaxed fairness constraint and tightens it as the algorithm progresses, facilitating convergence.\n",
    "3. **Ensured Assignment**: Implements a fallback mechanism to guarantee cluster assignment.\n",
    "4. **Flexibility**: Easy integration with scikit-learn tools.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How It Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FairKMeans` algorithm modifies the traditional K-Means clustering process with fairness-aware adjustments during the assignment step. The workflow is as follows:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Randomly select `n_clusters` data points as the initial centroids.\n",
    "   - Compute the global ratios of each value in the sensitive feature.\n",
    "\n",
    "2. **Cluster Assignment**:\n",
    "   - Assign data points to clusters based on distance, subject to fairness constraints.\n",
    "   - The fairness constraint ensures that the ratio of sensitive groups in each cluster stays within a dynamic tolerance of the global ratio.\n",
    "\n",
    "3. **Centroid Update**:\n",
    "   - Update cluster centroids based on the mean of assigned points.\n",
    "   - Reassign points to clusters to ensure fairness after centroid updates.\n",
    "\n",
    "4. **Convergence Check**:\n",
    "   - Stop when the change in centroids falls below a specified tolerance or after a maximum number of iterations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "def __init__(self, n_clusters=3, max_iter=100, tol=1e-4, random_state=42):\n",
    "    self.n_clusters = n_clusters\n",
    "    self.max_iter = max_iter\n",
    "    self.tol = tol\n",
    "    self.random_state = random_state\n",
    "    self.fairness_tolerance = 0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Parameters:**\n",
    "  - `n_clusters`: Number of clusters to form.\n",
    "  - `max_iter`: Maximum number of iterations to perform.\n",
    "  - `tol`: Convergence threshold for centroid movement.\n",
    "  - `random_state`: Random seed for reproducibility.\n",
    "  - `fairness_tolerance`: Maximum allowable deviation from global ratios for sensitive groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global Ratios:**\n",
    " - Compute the proportion of each sensitive group in the entire dataset.\n",
    "\n",
    "**Centroid Initialization:**\n",
    " - Randomly select `n_clusters` points from the data as initial centroids.\n",
    "\n",
    "**Cluster Assignment Initialization:**\n",
    " - `labels` stores the cluster assignment for each data point\n",
    " - `prev_labels` stores the previous cluster assignment for each data point (for debugging purposes)\n",
    " - `sensitive_counts` stores the counts of each sensitive feature value in each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster assignment with fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Point assignment:**  \n",
    "   Each data point is assigned to the nearest cluster that satisfies the fairness constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Fairness check:**  \n",
    "   Fairness is validated by comparing the current sensitive group ratio in a cluster to the global ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Centroid update:**  \n",
    "   After assignment, cluster centroids are updated based on the mean of assigned points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Reassigning points after centroid update:**  \n",
    "    After centroids update, reassign points to clusters to enforce fairness under new conditions (new centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Fall-back assignment:**  \n",
    "    If no cluster satisfies the fairness constraint, the data point is assigned to the nearest cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Convergence check:**  \n",
    "   If the change in centroids falls below the tolerance, the algorithm stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. FAIRNESS CHECK\n",
    "def _check_fairness(self, cluster, sensitive_value, sensitive_counts, total_in_cluster, global_ratios, iteration):\n",
    "    current_ratio = (sensitive_counts[cluster][sensitive_value] + 1) / (total_in_cluster + 1)\n",
    "    target_ratio = global_ratios[sensitive_value]\n",
    "    fairness_penalty = abs(current_ratio - target_ratio)\n",
    "    dynamic_tolerance = self.fairness_tolerance / (iteration + 1) ** 0.5\n",
    "    return fairness_penalty <= dynamic_tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, sensitive_feature):\n",
    "    X = check_array(X)\n",
    "    sensitive_feature = np.array(sensitive_feature)\n",
    "\n",
    "    # Global Ratios\n",
    "    global_ratios = {\n",
    "        value: np.sum(sensitive_feature == value) / len(sensitive_feature)\n",
    "        for value in np.unique(sensitive_feature)\n",
    "    }\n",
    "\n",
    "    # Centroid Initialization\n",
    "    rng = np.random.RandomState(self.random_state)\n",
    "    indices = rng.choice(len(X), self.n_clusters, replace=False)\n",
    "    self.cluster_centers_ = X[indices]\n",
    "\n",
    "    # Cluster Assignment Initialization\n",
    "    labels = np.zeros(X.shape[0], dtype=int)\n",
    "    prev_labels = labels.copy()\n",
    "    sensitive_counts = {i: {val: 0 for val in np.unique(sensitive_feature)} for i in range(self.n_clusters)}\n",
    "\n",
    "    for iteration in range(self.max_iter):\n",
    "        print(f\"\\nIteration {iteration + 1}\")\n",
    "        \n",
    "        # 1. POINT ASSIGNMENT\n",
    "        for i, sample in enumerate(X):\n",
    "            distances = np.linalg.norm(self.cluster_centers_ - sample, axis=1)  # Compute the distance from data point to all centroids                \n",
    "            sorted_clusters = np.argsort(distances)                             # Sort clusters by distance                   \n",
    "            for cluster in sorted_clusters:\n",
    "                total_in_cluster = sum(sensitive_counts[cluster].values())      # If the cluster is empty, automatically assign the point                              \n",
    "                # 2. FAIRNESS CHECK\n",
    "                if total_in_cluster == 0 or self._check_fairness(cluster, sensitive_feature[i], sensitive_counts, total_in_cluster, global_ratios, iteration):\n",
    "                    labels[i] = cluster                                         # Assign the data point to the first cluster that satisfies the fairness constraint\n",
    "                    sensitive_counts[cluster][sensitive_feature[i]] += 1        # Increment count for sensitive value in the assigned cluster\n",
    "                    break\n",
    "\n",
    "        # 3. CENTROID UPDATE            \n",
    "        new_centroids = np.array([\n",
    "            X[labels == i].mean(axis=0) if np.any(labels == i) else self.cluster_centers_[i]\n",
    "            for i in range(self.n_clusters)\n",
    "        ])\n",
    "\n",
    "        # 4. REASSIGNING POINTS AFTER CENTROID UPDATE\n",
    "        for i, sample in enumerate(X):\n",
    "            distances = np.linalg.norm(new_centroids - sample, axis=1)\n",
    "            sorted_clusters = np.argsort(distances)\n",
    "            assigned = False\n",
    "            for cluster in sorted_clusters:\n",
    "                total_in_cluster = sum(sensitive_counts[cluster].values())\n",
    "                if self._check_fairness(cluster, sensitive_feature[i], sensitive_counts, total_in_cluster, global_ratios, iteration):\n",
    "                    labels[i] = cluster\n",
    "                    sensitive_counts[cluster][sensitive_feature[i]] += 1\n",
    "                    assigned = True\n",
    "                    break\n",
    "                \n",
    "            # 5. FALL-BACK ASSIGNMENT\n",
    "            if not assigned:\n",
    "                cluster = sorted_clusters[0]\n",
    "                labels[i] = cluster\n",
    "                sensitive_counts[cluster][sensitive_feature[i]] += 1\n",
    "                print(f\"Forced assignment to Cluster {cluster}\")\n",
    "        \n",
    "        # 6. CONVERGENCE CHECK\n",
    "        centroid_shift = np.linalg.norm(self.cluster_centers_ - new_centroids)\n",
    "        if centroid_shift < self.tol:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    \"\"\"Predict the closest cluster each sample in X belongs to.\"\"\"\n",
    "    check_is_fitted(self, 'cluster_centers_')\n",
    "    X = check_array(X)\n",
    "    distances = np.linalg.norm(X[:, np.newaxis] - self.cluster_centers_, axis=2)\n",
    "    # Assigns each point to the nearest centroid\n",
    "    return np.argmin(distances, axis=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit(X, sensitive_feature)`\n",
    "Fits the model to the input data while enforcing fairness constraints.\n",
    "\n",
    "- **Parameters:**\n",
    "  - `X`: Training data (array-like of shape `(n_samples, n_features)`).\n",
    "  - `sensitive_feature`: Array of sensitive feature values (shape `(n_samples,)`).\n",
    "\n",
    "- **Returns:**\n",
    "  - `self`: The fitted model.\n",
    "\n",
    "### `_check_fairness(cluster, sensitive_value, ...)`\n",
    "Ensures that adding a point to a cluster does not violate fairness constraints.\n",
    "\n",
    "- **Returns:**\n",
    "  - `True` if fairness is maintained; otherwise, `False`.\n",
    "\n",
    "### `predict(X)`\n",
    "Assigns each sample in `X` to the nearest cluster.\n",
    "\n",
    "- **Parameters:**\n",
    "  - `X`: Input data (array-like of shape `(n_samples, n_features)`).\n",
    "\n",
    "- **Returns:**\n",
    "  - `labels`: Cluster assignments for each sample.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We use the `fetch_acs_income` dataset from `fairlearn.datasets`. This dataset contains demographic and income information, including a sensitive feature such as `race` or `sex`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from customFairKMeans import FairKMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fairlearn.datasets import fetch_acs_income\n",
    "from collections import Counter\n",
    "\n",
    "# Preprocess the given dataset\n",
    "def process_dataset(X, sensitive_feature):\n",
    "    \"\"\"\n",
    "    Encode all class values in X and sensitive_feature with numeric values and handle missing values.\n",
    "\n",
    "    Parameters:\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Input data.\n",
    "    sensitive_feature : array-like of shape (n_samples,)\n",
    "        Sensitive feature values for each sample.\n",
    "\n",
    "    Returns:\n",
    "    X_encoded : array-like of shape (n_samples, n_features)\n",
    "        Encoded input data.\n",
    "    sensitive_feature_encoded : array-like of shape (n_samples,)\n",
    "        Encoded sensitive feature.\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame for easier handling\n",
    "    X_df = pd.DataFrame(X)\n",
    "\n",
    "    # Drop columns with ALL NaN values\n",
    "    X_df = X_df.loc[:, X_df.notna().any(axis=0)]\n",
    "\n",
    "    # Fill NaN values in X: numeric columns with mean, categorical columns with \"unknown\"\n",
    "    X_df = X_df.apply(lambda col: col.fillna(col.mean()) if col.dtypes != 'object' else col.fillna(\"unknown\"))\n",
    "    # Encode categorical features in X\n",
    "    X_encoded = X_df.apply(lambda col: pd.factorize(col)[0] if col.dtypes == 'object' else col).values\n",
    "\n",
    "    # Fill NaN values and encode sensitive feature\n",
    "    sensitive_feature = pd.Series(sensitive_feature).fillna(\"unknown\")\n",
    "    sensitive_feature_encoded = pd.factorize(sensitive_feature)[0]\n",
    "\n",
    "    return X_encoded, sensitive_feature_encoded\n",
    "\n",
    "# Get cluster-wise distribution of sensitive features\n",
    "def cluster_sensitive_distribution(labels, sensitive_feature):\n",
    "    \"\"\"\n",
    "    Compute the distribution of sensitive features within each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    labels : array-like of shape (n_samples,)\n",
    "        Cluster labels for each sample.\n",
    "    sensitive_feature : array-like of shape (n_samples,)\n",
    "        Sensitive feature values for each sample.\n",
    "\n",
    "    Returns:\n",
    "    dict : A dictionary where keys are cluster IDs, and values are dictionaries\n",
    "           showing the count and percentage of each sensitive feature value.\n",
    "    \"\"\"\n",
    "\n",
    "    distribution = {}\n",
    "    sensitive_feature = np.array(sensitive_feature)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    for cluster in np.unique(labels):\n",
    "        cluster_indices = labels == cluster\n",
    "        total_in_cluster = np.sum(cluster_indices)\n",
    "\n",
    "        # Count sensitive features in the cluster\n",
    "        counts = Counter(sensitive_feature[cluster_indices])\n",
    "\n",
    "        # Convert counts to percentages\n",
    "        percentages = {key: (value / total_in_cluster) * 100 for key, value in counts.items()}\n",
    "\n",
    "        # Store both counts and percentages in the distribution\n",
    "        distribution[cluster] = {\n",
    "            \"counts\": counts,\n",
    "            \"percentages\": percentages\n",
    "        }\n",
    "\n",
    "    return distribution\n",
    "\n",
    "\n",
    "# Fetch the ACSIncome dataset\n",
    "data = fetch_acs_income()\n",
    "\n",
    "# Convert the dataset to a DataFrame for easier column access\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "sensitive_feature = X[\"SEX\"].values     # Extract the sensitive feature as a NumPy array\n",
    "X = X.drop(columns=[\"SEX\"]).values      # Drop the sensitive feature from the input data\n",
    "\n",
    "# Preprocess the given dataset \n",
    "X, sensitive_feature = process_dataset(X, sensitive_feature)\n",
    "\n",
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, sf_train, sf_test = train_test_split(\n",
    "    X, sensitive_feature, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize FairKMeans\n",
    "fair_kmeans = FairKMeans(n_clusters=3, max_iter=100, tol=1e-4, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "fair_kmeans.fit(X_train, sf_train)\n",
    "\n",
    "# Predict and evaluate FairKMeans on test set\n",
    "fair_predictions = fair_kmeans.predict(X_test)\n",
    "fair_distribution = cluster_sensitive_distribution(fair_predictions, sf_test)\n",
    "\n",
    "# Print sensitive feature distribution between clusters\n",
    "print(\"Sensitive Distribution (FairKMeans):\")\n",
    "for cluster, data in fair_distribution.items():\n",
    "    print(f\"  Cluster {cluster}:\")\n",
    "    for sf_value, percentage in data[\"percentages\"].items():\n",
    "        print(f\"    Sensitive Value {sf_value}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Global Ratios: {0: 0.47923400420546713, 1: 0.5207659957945329}\n",
    "\n",
    "Iteration 1\n",
    "Sensitive counts after assignment: {0: {0: 10, 1: 6}, 1: {0: 12, 1: 9}, 2: {0: 8, 1: 5}}\n",
    "Points Changing Clusters: 15\n",
    "Centroid Shift: 1.243\n",
    "\n",
    "Iteration 2\n",
    "Sensitive counts after assignment: {0: {0: 11, 1: 7}, 1: {0: 13, 1: 9}, 2: {0: 6, 1: 4}}\n",
    "Points Changing Clusters: 5\n",
    "Centroid Shift: 0.542\n",
    "\n",
    "...\n",
    "\n",
    "Convergence reached.\n",
    "\n",
    "Sensitive Distribution (FairKMeans):\n",
    "  Cluster 0:\n",
    "    Sensitive Value 0: 61.46%\n",
    "    Sensitive Value 1: 38.54%\n",
    "  Cluster 1:\n",
    "    Sensitive Value 1: 62.92%\n",
    "    Sensitive Value 0: 37.08%\n",
    "  Cluster 2:\n",
    "    Sensitive Value 1: 57.72%\n",
    "    Sensitive Value 0: 42.28%\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FairKMeans` algorithm demonstrates a critical balance between clustering fairness and quality. By enforcing fairness constraints during the clustering process, the algorithm ensures that sensitive groups are equitably represented across clusters. However, this fairness comes with trade-offs:  \n",
    "\n",
    "1. Fairness vs. Quality:  \n",
    "- As the fairness constraints tighten, the cluster centroids may no longer perfectly reflect the natural groupings in the data, potentially impacting the quality of the clusters.  \n",
    "- The algorithm ensures that sensitive groups are proportionately represented, but this can result in slightly higher within-cluster variance compared to traditional K-Means.  \n",
    "\n",
    "2. Impact of the number of clusters:\n",
    "- Increasing the number of clusters often makes it harder to maintain fairness. With more clusters, the ratios of sensitive groups within each cluster tend to deviate further from the global ratios.  \n",
    "- This phenomenon arises because the available data points are distributed across more clusters, reducing the flexibility to balance sensitive group proportions in every cluster.  \n",
    "\n",
    "3. Dynamic tolerance:\n",
    "- The use of a dynamic tolerance helps manage the trade-off by gradually tightening fairness constraints. Early iterations focus on forming clusters, while later iterations emphasize fairness adjustments.  \n",
    "\n",
    "By carefully tuning the number of clusters and fairness tolerance, the FairKMeans algorithm allows users to achieve a desirable balance between fairness and clustering quality, tailored to specific application needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
